{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4226b863-9ede-457a-91ac-6c3a8dc9935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "from spacy.training import Example\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from spacy.training.iob_utils import offsets_to_biluo_tags\n",
    "from spacy.scorer import Scorer\n",
    "from spacy import displacy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c7caeda-de74-4882-a670-e0b51fc41cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "def load_data(filepath, pure_text=False):\n",
    "    \"\"\"\n",
    "    Load the WNUT16 dataset, supporting both plain text and labeled modes.\n",
    "\n",
    "    Parameters:\n",
    "        filepath: Path to the data file.\n",
    "        pure_text: If True, only load plain text; otherwise, load data with BIO annotations.\n",
    "    Returns:\n",
    "        sentences: List of sentences.\n",
    "        labeled_sentences: List of labeled sentences in the format (text, {\"entities\": [(start, end, label)]}).\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    labeled_sentences = []\n",
    "    response = requests.get(filepath)\n",
    "    blocks = response.text.strip().split('\\n\\n')\n",
    "    \n",
    "    for block in blocks:\n",
    "        if not block.strip():\n",
    "            continue\n",
    "        lines = block.split('\\n')\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for line in lines:\n",
    "            if line.strip() and '\\t' in line:\n",
    "                token, label = line.split('\\t')\n",
    "                tokens.append(token.strip())\n",
    "                labels.append(label.strip())\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        if not tokens or not labels:\n",
    "            continue\n",
    "        \n",
    "        text = \" \".join(tokens)\n",
    "        if pure_text:\n",
    "            sentences.append(text)\n",
    "            labeled_sentences.append((text, {\"entities\": []}))\n",
    "            continue\n",
    "        \n",
    "        char_positions = []\n",
    "        current_pos = 0\n",
    "        for token in tokens:\n",
    "            char_positions.append(current_pos)\n",
    "            current_pos += len(token) + 1\n",
    "        \n",
    "        entities = []\n",
    "        seen_entities = set()\n",
    "        start = None\n",
    "        current_label = None\n",
    "        for i in range(len(tokens)):\n",
    "            label = labels[i]\n",
    "            if label.startswith(\"B-\"):\n",
    "                if current_label is not None:\n",
    "                    start_char = char_positions[start]\n",
    "                    end_char = char_positions[i - 1] + len(tokens[i - 1])\n",
    "                    entity_key = (start, i - 1, current_label)\n",
    "                    if entity_key not in seen_entities:\n",
    "                        seen_entities.add(entity_key)\n",
    "                        entities.append((start_char, end_char, current_label))\n",
    "                start = i\n",
    "                current_label = label[2:]  \n",
    "            elif label.startswith(\"I-\") and current_label:\n",
    "                expected_label = label[2:]\n",
    "                if expected_label != current_label:\n",
    "                    print(f\"Warning: Mismatched I- label found for '{text}' at position {i}: Expected {current_label}, but got {expected_label}\")\n",
    "                continue\n",
    "            else:\n",
    "                if current_label is not None:\n",
    "                    start_char = char_positions[start]\n",
    "                    end_char = char_positions[i - 1] + len(tokens[i - 1])\n",
    "                    entity_key = (start, i - 1, current_label)\n",
    "                    if entity_key not in seen_entities:\n",
    "                        seen_entities.add(entity_key)\n",
    "                        entities.append((start_char, end_char, current_label))\n",
    "                start = None\n",
    "                current_label = None\n",
    "        \n",
    "        if current_label is not None:\n",
    "            start_char = char_positions[start]\n",
    "            end_char = char_positions[-1] + len(tokens[-1])\n",
    "            entity_key = (start, len(tokens) - 1, current_label)\n",
    "            if entity_key not in seen_entities:\n",
    "                seen_entities.add(entity_key)\n",
    "                entities.append((start_char, end_char, current_label))\n",
    "        \n",
    "        sentences.append(text)\n",
    "        labeled_sentences.append((text, {\"entities\": entities}))\n",
    "    \n",
    "    return sentences, labeled_sentences\n",
    "    \n",
    "# 1.2 Sampling Function (Confidence-Based and Random Selection)\n",
    "def sample_sentences(unlabeled_sentences, n_confidence, n_random, ner):\n",
    "    \"\"\"\n",
    "    Mix confidence-based and random selection of sentences for active learning sampling.\n",
    "\n",
    "    Parameters:\n",
    "        unlabeled_sentences: List of unlabeled sentences.\n",
    "        n_confidence: Number of sentences selected based on confidence.\n",
    "\n",
    "        n_random: Number of sentences selected randomly.\n",
    "\n",
    "        ner: NER model component.\n",
    "\n",
    "    Returns:\n",
    "        List of sampled sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    def compute_confidence_score(doc, ner):\n",
    "        \"\"\"\n",
    "        Compute confidence score for uncertainty sampling \"\"\"\n",
    "        entity_scores = []\n",
    "        try:\n",
    "            doc_with_entities = ner(doc)\n",
    "            spans = doc_with_entities.ents\n",
    "            model_output = ner.model.predict([doc])\n",
    "            logits = model_output.logits if hasattr(model_output, 'logits') else model_output\n",
    "            if isinstance(logits, (list, tuple, np.ndarray)) and len(logits) > 0:\n",
    "                logits = logits[0] if isinstance(logits[0], (list, tuple, np.ndarray)) else logits\n",
    "            else:\n",
    "                logits = np.ones((len(doc), len(ner.labels)))\n",
    "            logits_np = np.asarray(logits)\n",
    "            def numpy_softmax(x, axis=-1):\n",
    "                exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "                return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "            probs = numpy_softmax(logits_np, axis=-1)\n",
    "            for span in spans:\n",
    "                probs_span = probs[span.start:span.end]\n",
    "                max_prob = np.max(probs_span) if len(probs_span) > 0 else 1.0\n",
    "                weighted_prob = max_prob * (span.end - span.start)\n",
    "                entity_scores.append(weighted_prob)\n",
    "        except Exception as e:\n",
    "            entity_scores.append(1.0)\n",
    "        return sum(entity_scores) / len(entity_scores) if entity_scores else 1.0\n",
    "\n",
    "    random_samples = random.sample(unlabeled_sentences, min(n_random, len(unlabeled_sentences)))\n",
    "    remaining_sentences = [s for s in unlabeled_sentences if s not in random_samples]\n",
    "    \n",
    "    scores = []\n",
    "    for sentence in remaining_sentences:\n",
    "        doc = nlp.make_doc(sentence)\n",
    "        avg_score = compute_confidence_score(doc, ner)\n",
    "        scores.append((sentence, avg_score))\n",
    "    \n",
    "    scores.sort(key=lambda x: x[1])\n",
    "    print(\"\\nTop 5 sentences with the lowest confidence:\")\n",
    "    for sentence, score in scores[:5]:\n",
    "        print(f\"Sentence: {sentence},  Confidence Score: {score}\")\n",
    "    confidence_samples = [s[0] for s in scores[:n_confidence]]\n",
    "    return confidence_samples + random_samples\n",
    "\n",
    "\n",
    "# 1.3 Import and Process Annotated Data\n",
    "def import_annotated_data(nlp, annotated_file, priority, low_freq_categories=None):\n",
    "    \"\"\" \n",
    "    Import and process annotated data, converting it into SpaCy format while avoiding forced ordering.\n",
    "\n",
    "    Parameters: \n",
    "        nlp: SpaCy model.\n",
    "        annotated_file: Path to the annotated file (Label Studio format). \n",
    "        priority: Dictionary of entity priorities (for reference only).\n",
    " \n",
    "        low_freq_categories: List of low-frequency categories (prioritized for retention).\n",
    "\n",
    "    Returns:\n",
    "        List of newly annotated data in the format [(text, {\"entities\": [(start, end, label)]})].\n",
    "    \"\"\"\n",
    "    low_freq_categories = low_freq_categories or []\n",
    "    with open(annotated_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        annotated_data = json.load(f)\n",
    "    \n",
    "    new_labeled_data = []\n",
    "    total_entities = 0\n",
    "    ignored_entities = 0\n",
    "    for item in annotated_data:\n",
    "        if \"data\" in item and \"text\" in item[\"data\"]:\n",
    "            text = item[\"data\"][\"text\"]\n",
    "            annotations = item.get(\"annotations\", [{}])[0].get(\"result\", [])\n",
    "        elif \"text\" in item:\n",
    "            text = item[\"text\"]\n",
    "            annotations = item.get(\"annotations\", [{}])[0].get(\"result\", [])\n",
    "        else:\n",
    "            raise ValueError(f\"Failed to extract 'text' from annotated data, incorrect item format: {item}\")\n",
    "\n",
    "        doc = nlp.make_doc(text)\n",
    "        entities_list = []\n",
    "        for r in annotations:\n",
    "            char_start = r[\"value\"][\"start\"]\n",
    "            char_end = r[\"value\"][\"end\"]\n",
    "            label = r[\"value\"][\"labels\"][0]\n",
    "            total_entities += 1\n",
    "            span = doc.char_span(char_start, char_end, label=label, alignment_mode=\"expand\")\n",
    "            if span is not None:\n",
    "                token_start = span.start\n",
    "                token_end = span.end\n",
    "                aligned_char_start = doc[token_start].idx\n",
    "                aligned_char_end = doc[token_end-1].idx + len(doc[token_end-1].text) if token_end > token_start else aligned_char_start + len(doc[token_start].text)\n",
    "                entities_list.append((aligned_char_start, aligned_char_end, label))\n",
    "            else:\n",
    "                print(f\"Failed to convert character span, Sentence: {text}, Entity: {char_start, char_end, label}\")\n",
    "        \n",
    "        # Sort by start and end positions, but do not sort by priority.\n",
    "        entities_list.sort(key=lambda x: (x[0], x[1]))\n",
    "        \n",
    "        # Conflict resolution: Prioritize retaining entities of low-frequency categories to avoid forced removal.\n",
    "\n",
    "        entities = []\n",
    "        used_tokens = set()\n",
    "        for char_start, char_end, label in entities_list:\n",
    "            span = doc.char_span(char_start, char_end, label=label, alignment_mode=\"expand\")\n",
    "            if span is None:\n",
    "                continue\n",
    "            start = span.start\n",
    "            end = span.end\n",
    "            overlap = False\n",
    "            for i in range(start, end):\n",
    "                if i in used_tokens:\n",
    "                    overlap = True\n",
    "                    break\n",
    "            # If there is an overlap, check if it belongs to a low-frequency category.\n",
    "\n",
    "            if overlap:\n",
    "                # If the current entity is a low-frequency category, prioritize its retention.\n",
    "                if label in low_freq_categories:\n",
    "                    # Remove previously conflicting entities and reassign tokens.\n",
    "                    new_entities = []\n",
    "                    new_used_tokens = set()\n",
    "                    for e in entities:\n",
    "                        e_span = doc.char_span(e[0], e[1], label=e[2], alignment_mode=\"expand\")\n",
    "                        if e_span is None:\n",
    "                            continue\n",
    "                        e_start = e_span.start\n",
    "                        e_end = e_span.end\n",
    "                        e_overlap = False\n",
    "                        for j in range(e_start, e_end):\n",
    "                            if j in range(start, end):\n",
    "                                e_overlap = True\n",
    "                                break\n",
    "                        if not e_overlap or (e_overlap and e[2] not in low_freq_categories):\n",
    "                            new_entities.append(e)\n",
    "                            for j in range(e_start, e_end):\n",
    "                                new_used_tokens.add(j)\n",
    "                    entities = new_entities\n",
    "                    used_tokens = new_used_tokens\n",
    "                    overlap = False\n",
    "            if not overlap:\n",
    "                entities.append((char_start, char_end, label))\n",
    "                for i in range(start, end):\n",
    "                    used_tokens.add(i)\n",
    "        \n",
    "        try:\n",
    "            biluo_tags = offsets_to_biluo_tags(doc, entities)\n",
    "            valid_entities = []\n",
    "            for char_start, char_end, label in entities:\n",
    "                span = doc.char_span(char_start, char_end, label=label, alignment_mode=\"expand\")\n",
    "                if span and all(biluo_tags[i] != \"-\" for i in range(span.start, span.end)):\n",
    "                    valid_entities.append((char_start, char_end, label))\n",
    "                else:\n",
    "                    print(f\"Skipping misaligned entity, Sentence: '{text}', Entity: {char_start, char_end, label}\")\n",
    "                    ignored_entities += 1\n",
    "            new_labeled_data.append((text, {\"entities\": valid_entities}))\n",
    "        except ValueError as e:\n",
    "            print(f\"Error aligning entities, Sentence: '{text}': {e}\")\n",
    "            new_labeled_data.append((text, {\"entities\": []}))\n",
    "    \n",
    "    print(f\"Loaded {len(new_labeled_data)} new labeled samples.\")\n",
    "    print(f\"Total entities: {total_entities}, Ignored entities: {ignored_entities}, Ignored ratio: {ignored_entities/total_entities:.2%}\")\n",
    "    if new_labeled_data:\n",
    "        print(\"Sample:\", new_labeled_data[0])\n",
    "    return new_labeled_data\n",
    "\n",
    "# 1.4 Train the Model\n",
    "def train_model(nlp, train_data, other_pipes, optimizer, epochs=30, dropout=0.15):\n",
    "    \"\"\"\n",
    "    Train the model.\n",
    "\n",
    "    Parameters:\n",
    "        nlp: SpaCy model.\n",
    "\n",
    "        train_data: Training data in the format [(text, {\"entities\": [(start, end, label)]})].\n",
    "\n",
    "        other_pipes: Other pipelines to disable.\n",
    "\n",
    "        optimizer: Optimizer.\n",
    "\n",
    "        epochs: Number of training epochs.\n",
    "\n",
    "        dropout: Dropout rate.\n",
    "\n",
    "    Returns:\n",
    "        The trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting model training\") \n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        for i in range(epochs):\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            for text, annotations in train_data:\n",
    "                doc = nlp.make_doc(text)\n",
    "                valid_entities = [(start, end, label) for start, end, label in annotations[\"entities\"]\n",
    "                                  if start >= 0 and end <= len(doc.text) and start < end]\n",
    "                example = Example.from_dict(doc, {\"entities\": valid_entities})\n",
    "                nlp.update([example], drop=dropout, sgd=optimizer, losses=losses)\n",
    "            print(f\" {i + 1} : {losses}\")\n",
    "    return nlp\n",
    "    \n",
    "# 1.5 Evaluate the Model\n",
    "def evaluate_model(nlp, data, dataset_name=\"Dev\"):\n",
    "    \"\"\"\n",
    "    Evaluate model performance.\n",
    "\n",
    "    Parameters:\n",
    "        nlp: SpaCy model.\n",
    "\n",
    "        data: Evaluation data in the format [(text, {\"entities\": [(start, end, label)]})].\n",
    "\n",
    "        dataset_name: Name of the dataset (for printing purposes).\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        Evaluation scores.\n",
    "    \"\"\"\n",
    "    scorer = spacy.scorer.Scorer()\n",
    "    examples = []\n",
    "    for text, annotations in data:\n",
    "        doc = nlp(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        examples.append(example)\n",
    "    scorer.score(examples) \n",
    "    print(f\"\\n{dataset_name} Metrics：\")\n",
    "    print(f\"Overall：{scorer.scores}\")\n",
    "    print(\"\\nPer Entity Type：\")\n",
    "    for entity_type, metrics in scorer.scores[\"ents_per_type\"].items():\n",
    "        print(f\"{entity_type}: {metrics}\")\n",
    "    return scorer.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c5ff949-7c6c-484b-bb57-ca71e07bd674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data\n",
      "train: 2394, dev: 1000, test: 3850\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://raw.githubusercontent.com/aritter/twitter_nlp/master/data/annotated/wnut16/data/\"\n",
    "priority = {\n",
    "    \"company\": 1, \"facility\": 2, \"geo-loc\": 3, \"movie\": 4, \"musicartist\": 5,\n",
    "    \"person\": 6, \"product\": 7, \"sportsteam\": 8, \"tvshow\": 9, \"other\": 10\n",
    "}\n",
    "\n",
    "print(\"Preprocessing data\")\n",
    "files = [\"train\", \"dev\", \"test\"]\n",
    "data = {}\n",
    "for file in files:\n",
    "    file_url = f\"{base_url}{file}\"\n",
    "    sentences, labeled_sentences = load_data(file_url, pure_text=True)\n",
    "    data[file] = sentences\n",
    "print(f\"train: {len(data['train'])}, dev: {len(data['dev'])}, test: {len(data['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "917f2f61-7929-4533-8c46-d9ff1353a6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating model\n",
      "Initial model has been saved as 'initial_ner_model'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCreating model\")\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "entity_types = [\"company\", \"facility\", \"geo-loc\", \"movie\", \"musicartist\", \"other\", \"person\", \"product\", \"sportsteam\", \"tvshow\"]\n",
    "for label in entity_types:\n",
    "    ner.add_label(label)\n",
    "nlp.initialize()\n",
    "nlp.to_disk(\"initial_ner_model\")\n",
    "print(\"Initial model has been saved as 'initial_ner_model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "888d120c-1c05-4ebc-a91a-113b5490739e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating pseudo labels based on rules\n",
      "Generated 100 pseudo-labeled samples.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGenerating pseudo labels based on rules\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "matcher.add(\"person\", [[{\"IS_UPPER\": True}, {\"IS_UPPER\": True}]])\n",
    "matcher.add(\"company\", [[{\"TEXT\": {\"REGEX\": r\"(Inc\\.|Ltd\\.|Corp\\.)$\"}}]])\n",
    "matcher.add(\"facility\", [[{\"IS_UPPER\": True}, {\"LOWER\": {\"IN\": [\"stadium\", \"airport\", \"museum\"]}}]])\n",
    "\n",
    "\n",
    "geo_loc_dict = [\"new york\", \"london\", \"tokyo\", \"shanghai\", \"paris\", \"los angeles\", \"california\", \"texas\", \"florida\", \"chicago\"]\n",
    "movie_dict = [\"the matrix\", \"titanic\", \"inception\", \"star wars\", \"the godfather\", \"pulp fiction\", \"the avengers\"]\n",
    "\n",
    "\n",
    "phrase_matcher.add(\"geo-loc\", [nlp.make_doc(loc) for loc in geo_loc_dict])\n",
    "phrase_matcher.add(\"movie\", [nlp.make_doc(movie) for movie in movie_dict])\n",
    "\n",
    "\n",
    "excluded_tokens = [\"RT\", \":D\", \"...\", \"ALL\", \"FML\", \"KK\", \"TIME\", \"LOW\", \"I\", \"WARNING\", \"do\", \"IN\", \"MY\", \"IS\", \"DONE\"]\n",
    "\n",
    "sample_size = 100\n",
    "sampled_sentences = random.sample(data[\"train\"], min(sample_size, len(data[\"train\"])))\n",
    "train_data = []\n",
    "\n",
    "for sentence in sampled_sentences:\n",
    "    doc = nlp(sentence)\n",
    "    entities_with_priority = []\n",
    "    for match_id, start, end in matcher(doc):\n",
    "        label = nlp.vocab.strings[match_id]\n",
    "        if any(token in excluded_tokens for token in doc[start:end].text.split()):\n",
    "            continue\n",
    "        entities_with_priority.append((start, end, label, priority[label]))\n",
    "    for match_id, start, end in phrase_matcher(doc):\n",
    "        label = nlp.vocab.strings[match_id]\n",
    "        entities_with_priority.append((start, end, label, priority[label]))\n",
    "    \n",
    "    entities_with_priority.sort(key=lambda x: (x[0], x[1], x[3]))\n",
    "    entities = []\n",
    "    used_tokens = set()\n",
    "    for start, end, label, _ in entities_with_priority:\n",
    "        if start < 0 or end > len(doc) or start >= end:\n",
    "            continue\n",
    "        overlap = any(i in used_tokens for i in range(start, end))\n",
    "        if not overlap:\n",
    "            char_start = doc[start].idx\n",
    "            char_end = doc[end-1].idx + len(doc[end-1].text) if end > start else char_start + len(doc[start].text)\n",
    "            if char_end > len(doc.text):\n",
    "                char_end = len(doc.text)\n",
    "            span = doc.char_span(char_start, char_end, label=label, alignment_mode=\"strict\")\n",
    "            if span:\n",
    "                entities.append((char_start, char_end, label))\n",
    "                for i in range(start, end):\n",
    "                    used_tokens.add(i)\n",
    "    try:\n",
    "        biluo_tags = offsets_to_biluo_tags(doc, entities)\n",
    "        valid_entities = [(char_start, char_end, label) for char_start, char_end, label in entities\n",
    "                          if char_start >= 0 and char_end <= len(doc.text) and char_start < char_end and\n",
    "                          all(biluo_tags[i] != \"-\" for i in range(doc.char_span(char_start, char_end, label=label, alignment_mode=\"expand\").start, \n",
    "                              doc.char_span(char_start, char_end, label=label, alignment_mode=\"expand\").end))]\n",
    "        train_data.append((sentence, {\"entities\": valid_entities}))\n",
    "    except ValueError as e:\n",
    "        print(f\"Error aligning entities, Sentence: '{sentence}': {e}\")\n",
    "        train_data.append((sentence, {\"entities\": []}))\n",
    "\n",
    "with open(\"initial_pseudo_labels.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False)\n",
    "print(f\"Generated {len(train_data)} pseudo-labeled samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd901977-a655-40eb-8d65-1056cf8a1ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Training\n",
      "Starting model training\n",
      " 1 : {'ner': 211.45419476459526}\n",
      " 2 : {'ner': 49.13733175813346}\n",
      " 3 : {'ner': 47.775095690906895}\n",
      " 4 : {'ner': 8.012485581540707}\n",
      " 5 : {'ner': 7.504210933557242}\n",
      " 6 : {'ner': 6.668474730856127}\n",
      " 7 : {'ner': 3.5153146960329082}\n",
      " 8 : {'ner': 0.860901114254744}\n",
      " 9 : {'ner': 1.9686555860688217}\n",
      " 10 : {'ner': 0.005218327837362067}\n",
      "Initial training completed. Model saved as  'initial_trained_model'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nInitial Training\")\n",
    "nlp = spacy.load(\"initial_ner_model\")\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "optimizer = nlp.begin_training()\n",
    "nlp = train_model(nlp, train_data, other_pipes, optimizer, epochs=10)\n",
    "nlp.to_disk(\"initial_trained_model\")\n",
    "unlabeled_sentences = [s for s in data[\"train\"] if s not in [t[0] for t in train_data]]\n",
    "with open(\"train_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False)\n",
    "with open(\"unlabeled_sentences.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(unlabeled_sentences, f, ensure_ascii=False)\n",
    "print(\"Initial training completed. Model saved as  'initial_trained_model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c871470b-6e7e-41d8-ba85-0a287f77aaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_first_iteration(iteration, model_name, export_file, n_confidence, n_random , epochs=15, dropout = 0.3):\n",
    "    \"\"\"\n",
    "    First Iteration: Sampling and exporting samples (without manual annotation file)\n",
    "    \n",
    "    Parameters:\n",
    "        iteration: Iteration number (used for printing steps)\n",
    "        model_name: Current model name (e.g., \"initial_trained_model\")\n",
    "        export_file: File name for exported samples (e.g., \"manual_samples_first.json\")\n",
    "        n_confidence: Number of confidence-based samples\n",
    "        n_random: Number of randomly selected samples\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(f\"\\nStep {2*iteration+5}：Active Learning Iteration {iteration} - Exporting data\")\n",
    "    global nlp, train_data, unlabeled_sentences\n",
    "    \n",
    "    # Load model\n",
    "    nlp = spacy.load(model_name)\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    \n",
    "    # Load train_data and unlabeled_sentences\n",
    "    with open(\"train_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        train_data = json.load(f)\n",
    "    with open(\"unlabeled_sentences.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        unlabeled_sentences = json.load(f)\n",
    "    \n",
    "    # Sample sentences\n",
    "    samples = sample_sentences(unlabeled_sentences, n_confidence, n_random, ner)  \n",
    "    with open(export_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([{\"text\": s} for s in samples], f, ensure_ascii=False)\n",
    "    unlabeled_sentences = [s for s in unlabeled_sentences if s not in samples]\n",
    "    with open(\"train_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(train_data, f, ensure_ascii=False)\n",
    "    with open(\"unlabeled_sentences.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(unlabeled_sentences, f, ensure_ascii=False)\n",
    "    print(f\"Exported {n_confidence + n_random} samples to '{export_file}'. Please annotate manually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b605ef4-e3d6-41d2-9015-3d69a3c33e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_subsequent_iteration(iteration, model_name, export_file, annotated_file, n_confidence, n_random, save_model_name, epochs=30, dropout=0.15, low_freq_categories=None, dev_url=None, target_f1=85.0, max_iterations=10, f1_history=None):\n",
    "    \"\"\"\n",
    "    Subsequent Iterations: Sampling, importing annotated data, training (single iteration), and evaluating performance.\n",
    "\n",
    "    Parameters:\n",
    "    iteration: Iteration number (used for printing steps)\n",
    "    model_name: Current model name (e.g., \"initial_trained_model\" or \"active_learning_ner_model_iteration_1\")\n",
    "    export_file: File name for exported samples (e.g., \"uncertain_samples_iteration_1.json\")\n",
    "    annotated_file: Manually annotated file name (e.g., \"manual_annotated_first.json\" or \"annotated_samples_iteration_1.json\")\n",
    "    n_confidence: Number of confidence-based samples\n",
    "    n_random: Number of randomly selected samples\n",
    "    save_model_name: Name of the saved model (e.g., \"active_learning_ner_model_iteration_1\")\n",
    "    epochs: Number of training epochs\n",
    "    \"\"\"\n",
    "    print(f\"\\nStep {iteration}：Active Learning Iteration {iteration} - Sampling, Training\")\n",
    "    global nlp, train_data, unlabeled_sentences\n",
    "    \n",
    "    # Load model\n",
    "    nlp = spacy.load(model_name)\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    \n",
    "    # Load optimizer state\n",
    "    optimizer_file = f\"{model_name}_optimizer.pkl\"\n",
    "    if os.path.exists(optimizer_file):\n",
    "        with open(optimizer_file, \"rb\") as f:\n",
    "            optimizer = pickle.load(f)\n",
    "        print(f\"Loaded optimizer state from '{optimizer_file}'.\")\n",
    "    else:\n",
    "        optimizer = nlp.begin_training()\n",
    "        optimizer.learn_rate = 0.0001\n",
    "        print(\"Initialized new optimizer with learn_rate=0.0001.\")\n",
    "    \n",
    "    # Load train_data and unlabeled_sentences\n",
    "    with open(\"train_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        train_data = json.load(f)\n",
    "    with open(\"unlabeled_sentences.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        unlabeled_sentences = json.load(f)\n",
    "\n",
    "    # Sample sentences\n",
    "    samples = sample_sentences_subsequent(unlabeled_sentences, n_confidence, n_random, ner, low_freq_categories=low_freq_categories)\n",
    "    with open(export_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump([{\"text\": s} for s in samples], f, ensure_ascii=False)\n",
    "    unlabeled_sentences = [s for s in unlabeled_sentences if s not in samples]\n",
    "    with open(\"unlabeled_sentences.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(unlabeled_sentences, f, ensure_ascii=False)\n",
    "    print(f\"Exported {n_confidence + n_random} samples to '{export_file}'. Please annotate '{annotated_file}'，然后继续下一次迭代。\")\n",
    "    \n",
    "    # Check if annotated_file exists\n",
    "    if not os.path.exists(annotated_file):\n",
    "        raise FileNotFoundError(f\"Annotated file '{annotated_file}' not found. Please ensure it has been annotated.\")\n",
    "    \n",
    "    # Load new annotated data\n",
    "    new_labeled_data = import_annotated_data(nlp, annotated_file, priority, low_freq_categories=low_freq_categories if low_freq_categories is not None else [])\n",
    "    train_data.extend(new_labeled_data)\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Training data size:  {len(train_data)}\")\n",
    "    nlp = train_model(nlp, train_data, other_pipes, optimizer, epochs=epochs, dropout=dropout)\n",
    "    \n",
    "    # Save model and optimizer state\n",
    "    nlp.to_disk(save_model_name)\n",
    "    with open(f\"{save_model_name}_optimizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(optimizer, f)\n",
    "    with open(\"train_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(train_data, f, ensure_ascii=False)\n",
    "    with open(\"unlabeled_sentences.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(unlabeled_sentences, f, ensure_ascii=False)\n",
    "    print(f\"Iteration {iteration} completed: Model saved as '{save_model_name}'。\")\n",
    "    return save_model_name, True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8988d473-4a74-4ebe-ae24-9f678ced201d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sentences_subsequent(unlabeled_sentences, n_confidence, n_random, ner, low_freq_categories=None):\n",
    "    \"\"\"\n",
    "    Mix confidence-based and random sentence sampling, with support for prioritizing low-frequency categories.\n",
    "\n",
    "    Parameters:\n",
    "        unlabeled_sentences: List of unlabeled sentences\n",
    "        n_confidence: Number of confidence-based samples\n",
    "        n_random: Number of randomly selected samples\n",
    "        ner: NER model component\n",
    "\n",
    "    Returns:\n",
    "        List of selected samples\n",
    "    \"\"\"\n",
    "    def compute_confidence_score(doc, ner):\n",
    "        entity_scores = []\n",
    "        entities_detected = []\n",
    "        try:\n",
    "            doc_with_entities = ner(doc)\n",
    "            spans = doc_with_entities.ents\n",
    "            model_output = ner.model.predict([doc])\n",
    "            logits = model_output.logits if hasattr(model_output, 'logits') else model_output\n",
    "            if isinstance(logits, (list, tuple, np.ndarray)) and len(logits) > 0:\n",
    "                logits = logits[0] if isinstance(logits[0], (list, tuple, np.ndarray)) else logits\n",
    "            else:\n",
    "                logits = np.ones((len(doc), len(ner.labels)))\n",
    "            logits_np = np.asarray(logits)\n",
    "            def numpy_softmax(x, axis=-1):\n",
    "                exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "                return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "            probs = numpy_softmax(logits_np, axis=-1)\n",
    "            for span in spans:\n",
    "                probs_span = probs[span.start:span.end]\n",
    "                max_prob = np.max(probs_span) if len(probs_span) > 0 else 1.0\n",
    "                weighted_prob = max_prob * (span.end - span.start)\n",
    "                entity_scores.append(weighted_prob)\n",
    "                entities_detected.append(span.label_)\n",
    "        except Exception as e:\n",
    "            entity_scores.append(1.0)\n",
    "        return sum(entity_scores) / len(entity_scores) if entity_scores else 1.0, entities_detected\n",
    "\n",
    "    # Random sampling\n",
    "    random_samples = random.sample(unlabeled_sentences, min(n_random, len(unlabeled_sentences)))\n",
    "    remaining_sentences = [s for s in unlabeled_sentences if s not in random_samples]\n",
    "    \n",
    "    # Prioritize sentences containing low-frequency categories\n",
    "    low_freq_samples = []\n",
    "    other_samples = []\n",
    "    \n",
    "    for sentence in remaining_sentences:\n",
    "        doc = nlp.make_doc(sentence)\n",
    "        avg_score, entities_detected = compute_confidence_score(doc, ner)\n",
    "        # Check if the sentence contains entities from low-frequency categories\n",
    "        if low_freq_categories and any(entity in low_freq_categories for entity in entities_detected):\n",
    "            low_freq_samples.append((sentence, avg_score))\n",
    "        else:\n",
    "            other_samples.append((sentence, avg_score))\n",
    "    \n",
    "    # Sort by confidence score\n",
    "    low_freq_samples.sort(key=lambda x: x[1])  \n",
    "    other_samples.sort(key=lambda x: x[1])\n",
    "    \n",
    "    # Prioritize sampling sentences from low-frequency categories\n",
    "    n_low_freq = min(len(low_freq_samples), int(n_confidence * 0.8)) \n",
    "    n_other = n_confidence - n_low_freq\n",
    "    \n",
    "    scores = low_freq_samples + other_samples\n",
    "    print(\"\\nTop 5 sentences with the lowest confidence：\")\n",
    "    for sentence, score in scores[:5]:\n",
    "        print(f\"Sentence: {sentence}, Confidence Score: {score}\")\n",
    "    \n",
    "    confidence_samples = [s[0] for s in low_freq_samples[:n_low_freq]] + [s[0] for s in other_samples[:n_other]]\n",
    "    \n",
    "    if low_freq_categories:\n",
    "        target_min_count = 30  \n",
    "        category_counts = {cat: 0 for cat in low_freq_categories}\n",
    "        \n",
    "        for sentence in confidence_samples:\n",
    "            doc = nlp.make_doc(sentence)\n",
    "            doc_with_entities = ner(doc)\n",
    "            for ent in doc_with_entities.ents:\n",
    "                if ent.label_ in low_freq_categories:\n",
    "                    category_counts[ent.label_] += 1\n",
    "        \n",
    "        print(\"\\nLow-Frequency Category Sampling Statistics:\")\n",
    "        for cat, count in category_counts.items():\n",
    "            print(f\"{cat}: {count} samples\")\n",
    "            if count < target_min_count:\n",
    "                print(f\"Warning: The number of samples for category {cat} ({count}) did not reach the target ({target_min_count})，. Resampling is recommended.\")\n",
    "    \n",
    "    return confidence_samples + random_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eed184cc-df14-47ad-be4f-44d84f563351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 7：Active Learning Iteration 1 - Exporting data\n",
      "\n",
      "Top 5 sentences with the lowest confidence:\n",
      "Sentence: Chicago Blackhawks general manager Stan Bowman said Cristobal Huet will be gone in two weeks ... http://fan.ac/Ydv #NHL,  Confidence Score: 0.1\n",
      "Sentence: HAVE YOU HEARD DJ STRATEGY IS NOW AT VISIONS LOUNGE IN HICKORY , NC ON WEDNESDAY ; S NIGHTS ... WOW !! DONT MISS IT !,  Confidence Score: 0.1\n",
      "Sentence: @MakeDaPussyDrip ALL DAY,  Confidence Score: 0.1\n",
      "Sentence: just whooped st . francis preps asssss . and i scored a goal :) and its friday . and i have no homeworkkk . SICK LIFEEEE,  Confidence Score: 0.1\n",
      "Sentence: RT @TheOutlawz : EVERYBODY WHO WANT A FOLLOW FROM ME FOR FOLLOW FRIDAY LET ME KNOW !! WE AINT LIKE THESE HOLLYWOOD AZZ RAPPERS WE FOLLOW BACK !,  Confidence Score: 0.13999999999999999\n",
      "Exported 100 samples to 'manual_samples_1.json'. Please annotate manually.\n"
     ]
    }
   ],
   "source": [
    "run_first_iteration(\n",
    "    iteration=1,\n",
    "    model_name=\"initial_trained_model\",\n",
    "    export_file=\"manual_samples_1.json\",\n",
    "    n_confidence=10,\n",
    "    n_random=90,\n",
    "    epochs=20,\n",
    "    dropout=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19a648dc-d054-4031-b7f8-6f10bb34a268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2：Active Learning Iteration 2 - Sampling, Training\n",
      "Initialized new optimizer with learn_rate=0.0001.\n",
      "\n",
      "Top 5 sentences with the lowest confidence：\n",
      "Sentence: @SammieLynnsMom @tg10781 they will be all done by Sunday trust me *wink*, Confidence Score: 1.0\n",
      "Sentence: Made it back home to GA . It sucks not to be at Disney world , but its good to be home . Time to start planning the next Disney World trip ., Confidence Score: 1.0\n",
      "Sentence: ' Breaking Dawn ' Returns to Vancouver on January 11th http://bit.ly/dbDMs8, Confidence Score: 1.0\n",
      "Sentence: @ls_n perhaps , but folks may find something in the gallery that is helpful in their day-to-day work as well . Even just to use it ., Confidence Score: 1.0\n",
      "Sentence: @Carr0t aye been tonight - excellent, Confidence Score: 1.0\n",
      "Exported 100 samples to 'manual_samples_2.json'. Please annotate 'manual_annotated_05_01.json'，然后继续下一次迭代。\n",
      "Loaded 100 new labeled samples.\n",
      "Total entities: 81, Ignored entities: 0, Ignored ratio: 0.00%\n",
      "Sample: ('Chicago Blackhawks general manager Stan Bowman said Cristobal Huet will be gone in two weeks ... http://fan.ac/Ydv #NHL', {'entities': [(0, 18, 'sportsteam'), (35, 46, 'person'), (52, 66, 'person')]})\n",
      "Training data size:  200\n",
      "Starting model training\n",
      " 1 : {'ner': 2050.275392306203}\n",
      " 2 : {'ner': 174.95521349134108}\n",
      " 3 : {'ner': 173.26457598519355}\n",
      " 4 : {'ner': 170.7291988864116}\n",
      " 5 : {'ner': 169.79652462052752}\n",
      " 6 : {'ner': 168.5910525570246}\n",
      " 7 : {'ner': 178.04257534633726}\n",
      " 8 : {'ner': 159.326785204165}\n",
      " 9 : {'ner': 158.88143371644708}\n",
      " 10 : {'ner': 155.62347040101992}\n",
      " 11 : {'ner': 166.37225100105377}\n",
      " 12 : {'ner': 187.94113917304833}\n",
      " 13 : {'ner': 137.21430555721682}\n",
      " 14 : {'ner': 135.97508457998825}\n",
      " 15 : {'ner': 134.89252549142196}\n",
      " 16 : {'ner': 126.27501783959731}\n",
      " 17 : {'ner': 127.84336996909667}\n",
      " 18 : {'ner': 125.62034655543735}\n",
      " 19 : {'ner': 115.74233972729854}\n",
      " 20 : {'ner': 112.29791840834453}\n",
      "Iteration 2 completed: Model saved as 'active_learning_ner_model_iteration_1'。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('active_learning_ner_model_iteration_1', True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_subsequent_iteration(\n",
    "    iteration=2,\n",
    "    model_name=\"initial_trained_model\",\n",
    "    export_file=\"manual_samples_2.json\",\n",
    "    annotated_file=\"manual_annotated_05_01.json\",\n",
    "    n_confidence=10,\n",
    "    n_random=90,\n",
    "    save_model_name=\"active_learning_ner_model_iteration_1\",\n",
    "    epochs=20,\n",
    "    dropout=0.3\n",
    ")\n",
    "###1-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8076cb75-5b03-47fc-88db-8966079bf715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3：Active Learning Iteration 3 - Sampling, Training\n",
      "Loaded optimizer state from 'active_learning_ner_model_iteration_1_optimizer.pkl'.\n",
      "\n",
      "Top 5 sentences with the lowest confidence：\n",
      "Sentence: CLUB BLU tonite ...... 90 's music .. oldskool night wiith dj finese, Confidence Score: 0.1\n",
      "Sentence: Jan Brewer : Beheadings ? http://bit.ly/a2LBMP, Confidence Score: 0.1\n",
      "Sentence: A Twisted two nighter in London NOVEMBER 18 ... Younger Brother ( Live) , Shpongle ( Simon Posford DJ set ) and other ... http://fb.me/Hqwb7kzG, Confidence Score: 0.1\n",
      "Sentence: I just remembered this week is The Heatwave 7th birthday ... meant to make tonight 's Brixton bashment our birthday party but I forgot LOL, Confidence Score: 0.1\n",
      "Sentence: Costa Rican group CocoFunka power this week 's Indiesent Exposure http://ht.ly/2G4nS by @fuseboxradio on @planetill, Confidence Score: 0.1\n",
      "Exported 100 samples to 'manual_samples_3.json'. Please annotate 'manual_annotated_05_02.json'，然后继续下一次迭代。\n",
      "Loaded 100 new labeled samples.\n",
      "Total entities: 66, Ignored entities: 0, Ignored ratio: 0.00%\n",
      "Sample: ('@SammieLynnsMom @tg10781 they will be all done by Sunday trust me *wink*', {'entities': []})\n",
      "Training data size:  300\n",
      "Starting model training\n",
      " 1 : {'ner': 243.14453785433156}\n",
      " 2 : {'ner': 232.15458169534355}\n",
      " 3 : {'ner': 233.14788498822475}\n",
      " 4 : {'ner': 230.40211767901627}\n",
      " 5 : {'ner': 216.72416601307637}\n",
      " 6 : {'ner': 218.54254998377095}\n",
      " 7 : {'ner': 201.92089083502864}\n",
      " 8 : {'ner': 200.3159204489435}\n",
      " 9 : {'ner': 194.51275236738917}\n",
      " 10 : {'ner': 192.98118511938418}\n",
      " 11 : {'ner': 187.58974608799102}\n",
      " 12 : {'ner': 191.97971946172206}\n",
      " 13 : {'ner': 179.12624605565438}\n",
      " 14 : {'ner': 172.9190824479426}\n",
      " 15 : {'ner': 159.3452336094627}\n",
      " 16 : {'ner': 171.24093172010262}\n",
      " 17 : {'ner': 149.37502587116379}\n",
      " 18 : {'ner': 136.69170390715777}\n",
      " 19 : {'ner': 162.30400914061798}\n",
      " 20 : {'ner': 155.80043948131845}\n",
      "Iteration 3 completed: Model saved as 'active_learning_ner_model_iteration_2'。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('active_learning_ner_model_iteration_2', True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_subsequent_iteration(\n",
    "    iteration=3,\n",
    "    model_name=\"active_learning_ner_model_iteration_1\",\n",
    "    export_file=\"manual_samples_3.json\",\n",
    "    annotated_file=\"manual_annotated_05_02.json\",\n",
    "    n_confidence=30,\n",
    "    n_random=70,\n",
    "    save_model_name=\"active_learning_ner_model_iteration_2\",\n",
    "    epochs=20,\n",
    "    dropout=0.3\n",
    ")\n",
    "###2-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e6f1d09-ba86-43c0-8533-4095a64777ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4：Active Learning Iteration 4 - Sampling, Training\n",
      "Loaded optimizer state from 'active_learning_ner_model_iteration_2_optimizer.pkl'.\n",
      "\n",
      "Top 5 sentences with the lowest confidence：\n",
      "Sentence: right . Five Star Day came out in may this year . why haven't ANY trailers come out here in Scotland yet ? i've waited about a year for this !, Confidence Score: 0.1\n",
      "Sentence: Justice Breyer 's About Face : Koran-Burning Is Constitutionally Protected After All via Atlas Shrugs http://tinyurl.com/39wg73o, Confidence Score: 0.1\n",
      "Sentence: http://bit.ly/aTTQYq When Pepsi to ring usually confirm to , winning a Nokia 5800 ?, Confidence Score: 0.1\n",
      "Sentence: Your paycheck may shrink next year , at least temporarily , if a vote on extending the Bush tax cuts slips past November . http://bit.ly/9y8fdS, Confidence Score: 0.1\n",
      "Sentence: we need some time alone we need to let it BREATH, Confidence Score: 0.1\n",
      "Exported 100 samples to 'manual_samples_4.json'. Please annotate 'manual_annotated_05_03.json'，然后继续下一次迭代。\n",
      "Loaded 100 new labeled samples.\n",
      "Total entities: 80, Ignored entities: 0, Ignored ratio: 0.00%\n",
      "Sample: (\"CLUB BLU tonite ...... 90 's music .. oldskool night wiith dj finese\", {'entities': [(0, 8, 'facility'), (59, 68, 'musicartist')]})\n",
      "Training data size:  400\n",
      "Starting model training\n",
      " 1 : {'ner': 293.1283808339336}\n",
      " 2 : {'ner': 268.9584905978158}\n",
      " 3 : {'ner': 274.5259875261818}\n",
      " 4 : {'ner': 246.04964940936992}\n",
      " 5 : {'ner': 245.71187684676187}\n",
      " 6 : {'ner': 232.6189890199207}\n",
      " 7 : {'ner': 257.99641821519543}\n",
      " 8 : {'ner': 214.43777046086984}\n",
      " 9 : {'ner': 203.6838718457875}\n",
      " 10 : {'ner': 219.9375584018025}\n",
      " 11 : {'ner': 215.24974479439402}\n",
      " 12 : {'ner': 206.44454491504914}\n",
      " 13 : {'ner': 190.62256146113288}\n",
      " 14 : {'ner': 194.37182568380626}\n",
      " 15 : {'ner': 183.65663824706655}\n",
      " 16 : {'ner': 155.6064945467179}\n",
      " 17 : {'ner': 155.23772592019228}\n",
      " 18 : {'ner': 208.40383951072224}\n",
      " 19 : {'ner': 169.17491721661543}\n",
      " 20 : {'ner': 168.58358014989585}\n",
      "Iteration 4 completed: Model saved as 'active_learning_ner_model_iteration_3'。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('active_learning_ner_model_iteration_3', True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_subsequent_iteration(\n",
    "    iteration=4,\n",
    "    model_name=\"active_learning_ner_model_iteration_2\",\n",
    "    export_file=\"manual_samples_4.json\",\n",
    "    annotated_file=\"manual_annotated_05_03.json\",\n",
    "    n_confidence=40,\n",
    "    n_random=60,\n",
    "    save_model_name=\"active_learning_ner_model_iteration_3\",\n",
    "    epochs=20,\n",
    "    dropout=0.3\n",
    ")\n",
    "###3-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "314f5f21-b882-4b3d-a4e8-dc4c289ed4cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5：Active Learning Iteration 5 - Sampling, Training\n",
      "Loaded optimizer state from 'active_learning_ner_model_iteration_3_optimizer.pkl'.\n",
      "\n",
      "Top 5 sentences with the lowest confidence：\n",
      "Sentence: 'On set filming Chris \" Rainy Days \" video . This is the final scene then we can finally edit it .', Confidence Score: 0.1\n",
      "Sentence: Just changed my twitter background , check it out ! Found it at http://twitrounds.com .. September 17 , 2010 , 2:52 pm, Confidence Score: 0.1\n",
      "Sentence: RT @eljmayes : Ladbrokes Labour Leadership Market- http://bit.ly/cD3Rn8 Ed Miliband 's odds have shortened significantly in the last week ., Confidence Score: 0.1\n",
      "Sentence: #news Dems to voters : You may hate us , but GOP is worse ( AP ) ( Yahoo ! ) : Share With Friends : | Latest Top Ne ... http://adpro.co/aQxQtY, Confidence Score: 0.1\n",
      "Sentence: Kristen is MY LIFE . She made me who i am today , I was a fucker before her , she fixed me . And so did Robert ., Confidence Score: 0.1\n",
      "Exported 120 samples to 'manual_samples_5.json'. Please annotate 'manual_annotated_05_04.json'，然后继续下一次迭代。\n",
      "Loaded 100 new labeled samples.\n",
      "Total entities: 97, Ignored entities: 0, Ignored ratio: 0.00%\n",
      "Sample: (\"right . Five Star Day came out in may this year . why haven't ANY trailers come out here in Scotland yet ? i've waited about a year for this !\", {'entities': [(92, 100, 'geo-loc')]})\n",
      "Training data size:  500\n",
      "Starting model training\n",
      " 1 : {'ner': 295.469378383862}\n",
      " 2 : {'ner': 274.47620491035156}\n",
      " 3 : {'ner': 279.42126254863075}\n",
      " 4 : {'ner': 257.69126222448995}\n",
      " 5 : {'ner': 237.70058141129405}\n",
      " 6 : {'ner': 224.03438216975889}\n",
      " 7 : {'ner': 218.1735253313454}\n",
      " 8 : {'ner': 205.8394495089032}\n",
      " 9 : {'ner': 198.7808711813368}\n",
      " 10 : {'ner': 193.68803203586015}\n",
      " 11 : {'ner': 198.8865244722987}\n",
      " 12 : {'ner': 171.70335687631913}\n",
      " 13 : {'ner': 149.08489123765386}\n",
      " 14 : {'ner': 128.48810651838136}\n",
      " 15 : {'ner': 146.96729080682118}\n",
      " 16 : {'ner': 118.61614795325114}\n",
      " 17 : {'ner': 101.50347845424426}\n",
      " 18 : {'ner': 105.79069695129569}\n",
      " 19 : {'ner': 97.62037382950882}\n",
      " 20 : {'ner': 97.75560202359657}\n",
      "Iteration 5 completed: Model saved as 'active_learning_ner_model_iteration_4'。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('active_learning_ner_model_iteration_4', True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_subsequent_iteration(\n",
    "    iteration=5,\n",
    "    model_name=\"active_learning_ner_model_iteration_3\",\n",
    "    export_file=\"manual_samples_5.json\",\n",
    "    annotated_file=\"manual_annotated_05_04.json\",\n",
    "    n_confidence=60,\n",
    "    n_random=60,\n",
    "    save_model_name=\"active_learning_ner_model_iteration_4\",\n",
    "    epochs=20,\n",
    "    dropout=0.25\n",
    ")\n",
    "###4-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd55b1ec-6c8c-4029-9fdf-73643d6c2f01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 6：Active Learning Iteration 6 - Sampling, Training\n",
      "Loaded optimizer state from 'active_learning_ner_model_iteration_4_optimizer.pkl'.\n",
      "\n",
      "Top 5 sentences with the lowest confidence：\n",
      "Sentence: George N . Parks , UMass band director , dies after performance in Ohio : George N . Parks , for 33 years the dire ... http://tinyurl.com/2femvgq, Confidence Score: 0.1\n",
      "Sentence: MD Jobs | Executive Pharmacist-Full Time Float-Ellicott City , MD Job ( MD ) http://bit.ly/daNH6v #Job #Hiring #MDJobs, Confidence Score: 0.1\n",
      "Sentence: Football game tonight with mariaelena , sarah , and brittany ! Mood : excited !! GOO WB WILDCATS !, Confidence Score: 0.1\n",
      "Sentence: RT @Quotealicious : Today , I saw a guy driving a Pepsi truck , drinking a Coke . MLIA #Quotealicious, Confidence Score: 0.1\n",
      "Sentence: Aggressive Kids With ADHD May Not Need Antipsychotic Meds http://t.co/JfGm0uH, Confidence Score: 0.1\n",
      "Exported 120 samples to 'manual_samples_6.json'. Please annotate 'manual_annotated_05_05.json'，然后继续下一次迭代。\n",
      "Loaded 120 new labeled samples.\n",
      "Total entities: 107, Ignored entities: 0, Ignored ratio: 0.00%\n",
      "Sample: ('\\'On set filming Chris \" Rainy Days \" video . This is the final scene then we can finally edit it .\\'', {'entities': [(16, 21, 'person'), (24, 34, 'other')]})\n",
      "Training data size:  620\n",
      "Starting model training\n",
      " 1 : {'ner': 264.716658536247}\n",
      " 2 : {'ner': 265.8164374328096}\n",
      " 3 : {'ner': 231.32124881711795}\n",
      " 4 : {'ner': 233.13327288438705}\n",
      " 5 : {'ner': 225.34340352953896}\n",
      " 6 : {'ner': 213.78381887749572}\n",
      " 7 : {'ner': 182.587659809529}\n",
      " 8 : {'ner': 185.0602833114515}\n",
      " 9 : {'ner': 158.03452399673012}\n",
      " 10 : {'ner': 167.0523288613312}\n",
      " 11 : {'ner': 141.96214307106624}\n",
      " 12 : {'ner': 125.58760822679554}\n",
      " 13 : {'ner': 112.06794184986428}\n",
      " 14 : {'ner': 128.31523281700933}\n",
      " 15 : {'ner': 96.75720605098157}\n",
      " 16 : {'ner': 112.50778631680669}\n",
      " 17 : {'ner': 71.82111720130611}\n",
      " 18 : {'ner': 101.16341353427583}\n",
      " 19 : {'ner': 78.22965188051629}\n",
      " 20 : {'ner': 89.97299227534454}\n",
      "Iteration 6 completed: Model saved as 'active_learning_ner_model_iteration_5'。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('active_learning_ner_model_iteration_5', True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_subsequent_iteration(\n",
    "    iteration=6,\n",
    "    model_name=\"active_learning_ner_model_iteration_4\",\n",
    "    export_file=\"manual_samples_6.json\",\n",
    "    annotated_file=\"manual_annotated_05_05.json\",\n",
    "    n_confidence=90,\n",
    "    n_random=30,\n",
    "    save_model_name=\"active_learning_ner_model_iteration_5\",\n",
    "    epochs=20,\n",
    "    dropout=0.25\n",
    ")\n",
    "###5-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5225bd99-8010-4386-b0b5-7f6ee486e8dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 7：Active Learning Iteration 7 - Sampling, Training\n",
      "Loaded optimizer state from 'active_learning_ner_model_iteration_5_optimizer.pkl'.\n",
      "\n",
      "Top 5 sentences with the lowest confidence：\n",
      "Sentence: @LinnySmit Linny Linny Linny . U are something lady ... Well that story has been on hold for weeks now hasn't it ? I want to get on it this &gt; &gt;, Confidence Score: 0.1\n",
      "Sentence: The last time I chatted with Kyle was June . The last time we emailed was August ., Confidence Score: 0.1\n",
      "Sentence: :( RT @themaine Who is coming to the show tomorrow in Hawaii ?, Confidence Score: 0.1\n",
      "Sentence: Dems to voters : You may hate us , but GOP is worse ( AP ) AP - With just six weeks to avoid a possible election catastrophe , http://tiny.ly/wc5, Confidence Score: 0.1\n",
      "Sentence: Don Mattingly will replace Joe Torre as LA Dodgers manager after this season, Confidence Score: 0.1\n",
      "Exported 130 samples to 'manual_samples_7.json'. Please annotate 'manual_annotated_05_06.json'，然后继续下一次迭代。\n",
      "Loaded 120 new labeled samples.\n",
      "Total entities: 176, Ignored entities: 0, Ignored ratio: 0.00%\n",
      "Sample: ('George N . Parks , UMass band director , dies after performance in Ohio : George N . Parks , for 33 years the dire ... http://tinyurl.com/2femvgq', {'entities': [(0, 16, 'person'), (19, 24, 'other'), (67, 71, 'geo-loc'), (74, 90, 'person')]})\n",
      "Training data size:  740\n",
      "Starting model training\n",
      " 1 : {'ner': 386.9958988197757}\n",
      " 2 : {'ner': 314.2947808392917}\n",
      " 3 : {'ner': 320.8229613919158}\n",
      " 4 : {'ner': 287.7339044486947}\n",
      " 5 : {'ner': 264.3683352717441}\n",
      " 6 : {'ner': 225.94022430843793}\n",
      " 7 : {'ner': 215.8960726990894}\n",
      " 8 : {'ner': 190.16974466116477}\n",
      " 9 : {'ner': 176.8863128199968}\n",
      " 10 : {'ner': 160.19843402253636}\n",
      " 11 : {'ner': 137.26768666074017}\n",
      " 12 : {'ner': 138.92556154240202}\n",
      " 13 : {'ner': 106.42029916593002}\n",
      " 14 : {'ner': 104.5599639253065}\n",
      " 15 : {'ner': 94.43135961572297}\n",
      " 16 : {'ner': 91.99134381370429}\n",
      " 17 : {'ner': 80.11112166370188}\n",
      " 18 : {'ner': 108.02996374518797}\n",
      " 19 : {'ner': 68.48958062046982}\n",
      " 20 : {'ner': 76.62009032076273}\n",
      "Iteration 7 completed: Model saved as 'active_learning_ner_model_iteration_6'。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('active_learning_ner_model_iteration_6', True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_subsequent_iteration(\n",
    "    iteration=7,\n",
    "    model_name=\"active_learning_ner_model_iteration_5\",\n",
    "    export_file=\"manual_samples_7.json\",\n",
    "    annotated_file=\"manual_annotated_05_06.json\",\n",
    "    n_confidence=100,\n",
    "    n_random=30,\n",
    "    save_model_name=\"active_learning_ner_model_iteration_6\",\n",
    "    epochs=20,\n",
    "    dropout=0.2\n",
    ")\n",
    "###6-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "154045b1-3db3-4b04-97ca-8386bb5f0efc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 8：Active Learning Iteration 8 - Sampling, Training\n",
      "Loaded optimizer state from 'active_learning_ner_model_iteration_6_optimizer.pkl'.\n",
      "\n",
      "Top 5 sentences with the lowest confidence：\n",
      "Sentence: 'Come to \" MRB @ 61 Roadhouse BBQ \" Sunday , October 10 from 12:00 pm to 4:00 pm . We are playing for the 61 Roadhouse ... http://fb.me/G1FbJGw1', Confidence Score: 0.1\n",
      "Sentence: Come to \" 6th Biannual 24 Hour Prayer Focus \" Saturday , November 13 from 10:00 am to 1:00 pm . Mark the Date !!! http://fb.me/JyYXPmql, Confidence Score: 0.1\n",
      "Sentence: Hurry up ! Santy will be Leaving in 2 days ! - #Eskorte and #Massasje i #Norge, Confidence Score: 0.1\n",
      "Sentence: Power nap . I need it . Its been a stressful week . I'm excited for xmas , haha . Goodnight . :), Confidence Score: 0.1\n",
      "Sentence: A Few Clouds and 69 F at Islip , Long Island Mac Arthur Airport , NY Winds are North at 16.1 MPH ( 14 KT) . The pressure is http://s1z.us/vf.htm, Confidence Score: 0.1\n",
      "\n",
      "Low-Frequency Category Sampling Statistics:\n",
      "company: 13 samples\n",
      "Warning: The number of samples for category company (13) did not reach the target (20)，. Resampling is recommended.\n",
      "facility: 7 samples\n",
      "Warning: The number of samples for category facility (7) did not reach the target (20)，. Resampling is recommended.\n",
      "geo-loc: 15 samples\n",
      "Warning: The number of samples for category geo-loc (15) did not reach the target (20)，. Resampling is recommended.\n",
      "movie: 2 samples\n",
      "Warning: The number of samples for category movie (2) did not reach the target (20)，. Resampling is recommended.\n",
      "musicartist: 4 samples\n",
      "Warning: The number of samples for category musicartist (4) did not reach the target (20)，. Resampling is recommended.\n",
      "person: 26 samples\n",
      "product: 5 samples\n",
      "Warning: The number of samples for category product (5) did not reach the target (20)，. Resampling is recommended.\n",
      "sportsteam: 4 samples\n",
      "Warning: The number of samples for category sportsteam (4) did not reach the target (20)，. Resampling is recommended.\n",
      "tvshow: 2 samples\n",
      "Warning: The number of samples for category tvshow (2) did not reach the target (20)，. Resampling is recommended.\n",
      "other: 29 samples\n",
      "Exported 150 samples to 'manual_samples_8.json'. Please annotate 'manual_annotated_05_07.json'，然后继续下一次迭代。\n",
      "Loaded 130 new labeled samples.\n",
      "Total entities: 158, Ignored entities: 0, Ignored ratio: 0.00%\n",
      "Sample: (\"@LinnySmit Linny Linny Linny . U are something lady ... Well that story has been on hold for weeks now hasn't it ? I want to get on it this &gt; &gt;\", {'entities': []})\n",
      "Training data size:  870\n",
      "Starting model training\n",
      " 1 : {'ner': 357.41696713970134}\n",
      " 2 : {'ner': 293.9438163299808}\n",
      " 3 : {'ner': 260.00796429431574}\n",
      " 4 : {'ner': 258.8849683840508}\n",
      " 5 : {'ner': 231.46012700658255}\n",
      " 6 : {'ner': 209.0803212126642}\n",
      " 7 : {'ner': 184.21111729952273}\n",
      " 8 : {'ner': 188.98602620509558}\n",
      " 9 : {'ner': 199.17621352052493}\n",
      " 10 : {'ner': 142.35301506699227}\n",
      " 11 : {'ner': 146.96578202356773}\n",
      " 12 : {'ner': 166.83395404816233}\n",
      " 13 : {'ner': 127.30930132574308}\n",
      " 14 : {'ner': 98.31363299068302}\n",
      " 15 : {'ner': 98.71170473999267}\n",
      " 16 : {'ner': 84.5602180741115}\n",
      " 17 : {'ner': 87.1150050594525}\n",
      " 18 : {'ner': 73.01289701996778}\n",
      " 19 : {'ner': 77.0809927170884}\n",
      " 20 : {'ner': 77.38116967923104}\n",
      "Iteration 8 completed: Model saved as 'active_learning_ner_model_iteration_7'。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('active_learning_ner_model_iteration_7', True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_subsequent_iteration(\n",
    "    iteration=8,\n",
    "    model_name=\"active_learning_ner_model_iteration_6\",\n",
    "    export_file=\"manual_samples_8.json\",\n",
    "    annotated_file=\"manual_annotated_05_07.json\",\n",
    "    n_confidence=120,\n",
    "    n_random=30,\n",
    "    save_model_name=\"active_learning_ner_model_iteration_7\",\n",
    "    epochs=20,\n",
    "    dropout=0.2,\n",
    "    low_freq_categories =  [\"company\", \"facility\", \"geo-loc\", \"movie\", \"musicartist\",\n",
    "    \"person\", \"product\", \"sportsteam\", \"tvshow\", \"other\"]\n",
    ")\n",
    "###7-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ffab61d-7bc6-4cff-a223-6107260fd6dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 9：Active Learning Iteration 9 - Sampling, Training\n",
      "Loaded optimizer state from 'active_learning_ner_model_iteration_7_optimizer.pkl'.\n",
      "\n",
      "Top 5 sentences with the lowest confidence：\n",
      "Sentence: First Day of Autumn Networking Mixer at Dick 's Carpet next T ... http://conta.cc/ap95gL via #constantcontact, Confidence Score: 0.1\n",
      "Sentence: Check this video out -- Three Days Grace - Break ( Official Music Video ) [ HQ ] http://t.co/GOwCLQJ via @youtube, Confidence Score: 0.1\n",
      "Sentence: Game tonight with Tessa . I get to see Seanzie if he goes :) . I get to see my boyfriend too &lt; 3 c :, Confidence Score: 0.1\n",
      "Sentence: @Loserface_Laura when mike lets me know , I will let you know . I mean everyone might just switch out a lot ., Confidence Score: 0.1\n",
      "Sentence: CAFE NINE TONIGHT AT 11 !!, Confidence Score: 0.1\n",
      "\n",
      "Low-Frequency Category Sampling Statistics:\n",
      "company: 4 samples\n",
      "Warning: The number of samples for category company (4) did not reach the target (20)，. Resampling is recommended.\n",
      "facility: 5 samples\n",
      "Warning: The number of samples for category facility (5) did not reach the target (20)，. Resampling is recommended.\n",
      "geo-loc: 5 samples\n",
      "Warning: The number of samples for category geo-loc (5) did not reach the target (20)，. Resampling is recommended.\n",
      "movie: 0 samples\n",
      "Warning: The number of samples for category movie (0) did not reach the target (20)，. Resampling is recommended.\n",
      "musicartist: 1 samples\n",
      "Warning: The number of samples for category musicartist (1) did not reach the target (20)，. Resampling is recommended.\n",
      "person: 22 samples\n",
      "product: 3 samples\n",
      "Warning: The number of samples for category product (3) did not reach the target (20)，. Resampling is recommended.\n",
      "sportsteam: 0 samples\n",
      "Warning: The number of samples for category sportsteam (0) did not reach the target (20)，. Resampling is recommended.\n",
      "tvshow: 1 samples\n",
      "Warning: The number of samples for category tvshow (1) did not reach the target (20)，. Resampling is recommended.\n",
      "other: 22 samples\n",
      "Exported 150 samples to 'manual_samples_9.json'. Please annotate 'manual_annotated_05_08.json'，然后继续下一次迭代。\n",
      "Loaded 150 new labeled samples.\n",
      "Total entities: 165, Ignored entities: 0, Ignored ratio: 0.00%\n",
      "Sample: ('\\'Come to \" MRB @ 61 Roadhouse BBQ \" Sunday , October 10 from 12:00 pm to 4:00 pm . We are playing for the 61 Roadhouse ... http://fb.me/G1FbJGw1\\'', {'entities': [(11, 33, 'other')]})\n",
      "Training data size:  1020\n",
      "Starting model training\n",
      " 1 : {'ner': 399.14561204653904}\n",
      " 2 : {'ner': 377.58968684532175}\n",
      " 3 : {'ner': 332.2439032868868}\n",
      " 4 : {'ner': 346.5890866681684}\n",
      " 5 : {'ner': 238.04290932654035}\n",
      " 6 : {'ner': 289.12388547994374}\n",
      " 7 : {'ner': 221.7058162626633}\n",
      " 8 : {'ner': 214.75227742683012}\n",
      " 9 : {'ner': 203.05443071108883}\n",
      " 10 : {'ner': 170.88179434813813}\n",
      " 11 : {'ner': 166.51390058861057}\n",
      " 12 : {'ner': 139.51277781997288}\n",
      " 13 : {'ner': 159.7356671088111}\n",
      " 14 : {'ner': 141.02231282528217}\n",
      " 15 : {'ner': 134.89470055059448}\n",
      " 16 : {'ner': 88.27438092856733}\n",
      " 17 : {'ner': 67.8255395379091}\n",
      " 18 : {'ner': 91.638078717605}\n",
      " 19 : {'ner': 81.13116756191282}\n",
      " 20 : {'ner': 80.74565255328977}\n",
      "Iteration 9 completed: Model saved as 'active_learning_ner_model_iteration_8'。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('active_learning_ner_model_iteration_8', True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_subsequent_iteration(\n",
    "    iteration=9,\n",
    "    model_name=\"active_learning_ner_model_iteration_7\",\n",
    "    export_file=\"manual_samples_9.json\",\n",
    "    annotated_file=\"manual_annotated_05_08.json\",\n",
    "    n_confidence=120,\n",
    "    n_random=30,\n",
    "    save_model_name=\"active_learning_ner_model_iteration_8\",\n",
    "    epochs=20,\n",
    "    dropout=0.2,\n",
    "    low_freq_categories =  [\"company\", \"facility\", \"geo-loc\", \"movie\", \"musicartist\",\n",
    "    \"person\", \"product\", \"sportsteam\", \"tvshow\", \"other\"]\n",
    ")\n",
    "###8-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e390ffc5-0170-46b8-9e41-0fa6538a157b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 10：Active Learning Iteration 10 - Sampling, Training\n",
      "Loaded optimizer state from 'active_learning_ner_model_iteration_8_optimizer.pkl'.\n",
      "\n",
      "Top 5 sentences with the lowest confidence：\n",
      "Sentence: i want a bath but do n't have a bath , shut up , sam 's coming tomorrow and steve and tanya will be round at 10am so go away you mean people, Confidence Score: 0.1\n",
      "Sentence: RT @TeamShaneDawson : RT If you think @AntiShaneDawson is a 10 year old without a penis to play with so they hate on Shane instead :( :' ), Confidence Score: 0.1\n",
      "Sentence: @KSSchro When I run over an animal I think of Disney movies &amp; that I just ran over someones brother , mom or dad . The full life montage helps, Confidence Score: 0.1\n",
      "Sentence: @DebVRuns hi pal !! How 's Hawaii ?? when are you heading home !, Confidence Score: 0.1\n",
      "Sentence: O happy day . My local grocery carries Hoegaarden ., Confidence Score: 0.1\n",
      "\n",
      "Low-Frequency Category Sampling Statistics:\n",
      "company: 1 samples\n",
      "Warning: The number of samples for category company (1) did not reach the target (20)，. Resampling is recommended.\n",
      "facility: 0 samples\n",
      "Warning: The number of samples for category facility (0) did not reach the target (20)，. Resampling is recommended.\n",
      "geo-loc: 3 samples\n",
      "Warning: The number of samples for category geo-loc (3) did not reach the target (20)，. Resampling is recommended.\n",
      "movie: 0 samples\n",
      "Warning: The number of samples for category movie (0) did not reach the target (20)，. Resampling is recommended.\n",
      "musicartist: 2 samples\n",
      "Warning: The number of samples for category musicartist (2) did not reach the target (20)，. Resampling is recommended.\n",
      "person: 9 samples\n",
      "Warning: The number of samples for category person (9) did not reach the target (20)，. Resampling is recommended.\n",
      "product: 1 samples\n",
      "Warning: The number of samples for category product (1) did not reach the target (20)，. Resampling is recommended.\n",
      "sportsteam: 1 samples\n",
      "Warning: The number of samples for category sportsteam (1) did not reach the target (20)，. Resampling is recommended.\n",
      "tvshow: 0 samples\n",
      "Warning: The number of samples for category tvshow (0) did not reach the target (20)，. Resampling is recommended.\n",
      "other: 6 samples\n",
      "Warning: The number of samples for category other (6) did not reach the target (20)，. Resampling is recommended.\n",
      "Exported 150 samples to 'manual_samples_10.json'. Please annotate 'manual_annotated_05_09.json'，然后继续下一次迭代。\n",
      "Loaded 150 new labeled samples.\n",
      "Total entities: 126, Ignored entities: 0, Ignored ratio: 0.00%\n",
      "Sample: (\"First Day of Autumn Networking Mixer at Dick 's Carpet next T ... http://conta.cc/ap95gL via #constantcontact\", {'entities': [(0, 36, 'other'), (40, 54, 'facility')]})\n",
      "Training data size:  1170\n",
      "Starting model training\n",
      " 1 : {'ner': 349.2461616096426}\n",
      " 2 : {'ner': 279.9839555312017}\n",
      " 3 : {'ner': 273.42735412108516}\n",
      " 4 : {'ner': 227.1456655895342}\n",
      " 5 : {'ner': 217.51932839072467}\n",
      " 6 : {'ner': 206.79569567961337}\n",
      " 7 : {'ner': 178.32294433052135}\n",
      " 8 : {'ner': 179.83278209194904}\n",
      " 9 : {'ner': 151.70835263996378}\n",
      " 10 : {'ner': 114.69572065332189}\n",
      " 11 : {'ner': 136.6216817625717}\n",
      " 12 : {'ner': 117.76651733085767}\n",
      " 13 : {'ner': 130.87412023074936}\n",
      " 14 : {'ner': 109.73259733711649}\n",
      " 15 : {'ner': 96.424496674738}\n",
      "Iteration 10 completed: Model saved as 'active_learning_ner_model_iteration_9'。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('active_learning_ner_model_iteration_9', True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_subsequent_iteration(\n",
    "    iteration=10,\n",
    "    model_name=\"active_learning_ner_model_iteration_8\",\n",
    "    export_file=\"manual_samples_10.json\",\n",
    "    annotated_file=\"manual_annotated_05_09.json\",\n",
    "    n_confidence=120,\n",
    "    n_random=30,\n",
    "    save_model_name=\"active_learning_ner_model_iteration_9\",\n",
    "    epochs=15,\n",
    "    dropout=0.2,\n",
    "    low_freq_categories =  [\"company\", \"facility\", \"geo-loc\", \"movie\", \"musicartist\",\n",
    "    \"person\", \"product\", \"sportsteam\", \"tvshow\", \"other\"]\n",
    ")\n",
    "###9-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bc16ce2-edc8-4d11-b53f-fe38c209622e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 11：Active Learning Iteration 11 - Sampling, Training\n",
      "Loaded optimizer state from 'active_learning_ner_model_iteration_9_optimizer.pkl'.\n",
      "\n",
      "Top 5 sentences with the lowest confidence：\n",
      "Sentence: @vogueglamGIRL Ah I know ! She is simply the best in The Sept Issue . My boyfriend 's aunt worked for Anna Wintor in NY ., Confidence Score: 0.1\n",
      "Sentence: Fuckk man ! I fuckinn missed @yelyahwilliams concert :( #shizz ! I wonder if they're coming to Arizona next year ??, Confidence Score: 0.1\n",
      "Sentence: Blegh , that fell through . Staffing agency had a miscommunication w\\the client . So two week temp assignment fell through unfortunately ., Confidence Score: 0.1\n",
      "Sentence: Anja Rubik Model in Lingerie of the Day http://f.ast.ly/DCaEM, Confidence Score: 0.1\n",
      "Sentence: @StarryEyedJoeJ like after I download a PSD from @ CherryPSDs what do I do ?!, Confidence Score: 0.1\n",
      "\n",
      "Low-Frequency Category Sampling Statistics:\n",
      "company: 3 samples\n",
      "Warning: The number of samples for category company (3) did not reach the target (30)，. Resampling is recommended.\n",
      "facility: 3 samples\n",
      "Warning: The number of samples for category facility (3) did not reach the target (30)，. Resampling is recommended.\n",
      "geo-loc: 4 samples\n",
      "Warning: The number of samples for category geo-loc (4) did not reach the target (30)，. Resampling is recommended.\n",
      "movie: 1 samples\n",
      "Warning: The number of samples for category movie (1) did not reach the target (30)，. Resampling is recommended.\n",
      "musicartist: 2 samples\n",
      "Warning: The number of samples for category musicartist (2) did not reach the target (30)，. Resampling is recommended.\n",
      "person: 8 samples\n",
      "Warning: The number of samples for category person (8) did not reach the target (30)，. Resampling is recommended.\n",
      "product: 0 samples\n",
      "Warning: The number of samples for category product (0) did not reach the target (30)，. Resampling is recommended.\n",
      "sportsteam: 3 samples\n",
      "Warning: The number of samples for category sportsteam (3) did not reach the target (30)，. Resampling is recommended.\n",
      "tvshow: 0 samples\n",
      "Warning: The number of samples for category tvshow (0) did not reach the target (30)，. Resampling is recommended.\n",
      "other: 11 samples\n",
      "Warning: The number of samples for category other (11) did not reach the target (30)，. Resampling is recommended.\n",
      "Exported 150 samples to 'manual_samples_11.json'. Please annotate 'manual_annotated_05_10.json'，然后继续下一次迭代。\n",
      "Loaded 150 new labeled samples.\n",
      "Total entities: 53, Ignored entities: 0, Ignored ratio: 0.00%\n",
      "Sample: (\"i want a bath but do n't have a bath , shut up , sam 's coming tomorrow and steve and tanya will be round at 10am so go away you mean people\", {'entities': [(49, 52, 'person'), (76, 81, 'person'), (86, 91, 'person')]})\n",
      "Training data size:  1320\n",
      "Starting model training\n",
      " 1 : {'ner': 204.05621739682132}\n",
      " 2 : {'ner': 194.85147347824235}\n",
      " 3 : {'ner': 169.48959920050362}\n",
      " 4 : {'ner': 131.7395844075077}\n",
      " 5 : {'ner': 143.65436147827185}\n",
      " 6 : {'ner': 123.41181764308608}\n",
      " 7 : {'ner': 118.12566755305248}\n",
      " 8 : {'ner': 129.530800842189}\n",
      " 9 : {'ner': 136.8421192358929}\n",
      " 10 : {'ner': 81.49009826159903}\n",
      " 11 : {'ner': 110.36461773099332}\n",
      " 12 : {'ner': 99.00049140181282}\n",
      " 13 : {'ner': 96.08828931561433}\n",
      " 14 : {'ner': 69.39750569260252}\n",
      " 15 : {'ner': 76.19494334282369}\n",
      "Iteration 11 completed: Model saved as 'active_learning_ner_model_iteration_10'。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('active_learning_ner_model_iteration_10', True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_subsequent_iteration(\n",
    "    iteration=11,\n",
    "    model_name=\"active_learning_ner_model_iteration_9\",\n",
    "    export_file=\"manual_samples_11.json\",\n",
    "    annotated_file=\"manual_annotated_05_10.json\",\n",
    "    n_confidence=100,\n",
    "    n_random=50,\n",
    "    save_model_name=\"active_learning_ner_model_iteration_10\",\n",
    "    epochs=15,\n",
    "    dropout=0.2,\n",
    "    low_freq_categories =  [\"company\", \"facility\", \"geo-loc\", \"movie\", \"musicartist\",\n",
    "    \"person\", \"product\", \"sportsteam\", \"tvshow\", \"other\"]\n",
    ")\n",
    "###10-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b18e0ef-2a62-446e-af7b-e22904f513ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 12：Active Learning Iteration 12 - Sampling, Training\n",
      "Loaded optimizer state from 'active_learning_ner_model_iteration_10_optimizer.pkl'.\n",
      "\n",
      "Top 5 sentences with the lowest confidence：\n",
      "Sentence: RT @dropolo Headed to da gump today alabama here I come &lt; &lt; come to shut it down broski .. fuck wit me..Parlae 's in house producer, Confidence Score: 0.1\n",
      "Sentence: @cprieboy At the end of the year ? Sometime in May . They are just starting at the beginning of August now instead of the middle ., Confidence Score: 0.1\n",
      "Sentence: Amazing . And encouraging . RT @SusannahFox : 36,000 #bluebutton downloads for VA health records in 1st 10 days !, Confidence Score: 0.1\n",
      "Sentence: The pope isn't really making much of an effort . He 's wearing the same clothes as yesterday ., Confidence Score: 0.1\n",
      "Sentence: ' Free ' ' Day 26 ' '' #nowplaying http://cpwr.me/c9GNpt, Confidence Score: 0.1\n",
      "\n",
      "Low-Frequency Category Sampling Statistics:\n",
      "company: 0 samples\n",
      "Warning: The number of samples for category company (0) did not reach the target (30)，. Resampling is recommended.\n",
      "facility: 0 samples\n",
      "Warning: The number of samples for category facility (0) did not reach the target (30)，. Resampling is recommended.\n",
      "geo-loc: 3 samples\n",
      "Warning: The number of samples for category geo-loc (3) did not reach the target (30)，. Resampling is recommended.\n",
      "movie: 0 samples\n",
      "Warning: The number of samples for category movie (0) did not reach the target (30)，. Resampling is recommended.\n",
      "musicartist: 1 samples\n",
      "Warning: The number of samples for category musicartist (1) did not reach the target (30)，. Resampling is recommended.\n",
      "person: 3 samples\n",
      "Warning: The number of samples for category person (3) did not reach the target (30)，. Resampling is recommended.\n",
      "product: 1 samples\n",
      "Warning: The number of samples for category product (1) did not reach the target (30)，. Resampling is recommended.\n",
      "sportsteam: 0 samples\n",
      "Warning: The number of samples for category sportsteam (0) did not reach the target (30)，. Resampling is recommended.\n",
      "tvshow: 0 samples\n",
      "Warning: The number of samples for category tvshow (0) did not reach the target (30)，. Resampling is recommended.\n",
      "other: 6 samples\n",
      "Warning: The number of samples for category other (6) did not reach the target (30)，. Resampling is recommended.\n",
      "Exported 100 samples to 'manual_samples_12.json'. Please annotate 'manual_annotated_05_11.json'，然后继续下一次迭代。\n",
      "Loaded 150 new labeled samples.\n",
      "Total entities: 57, Ignored entities: 0, Ignored ratio: 0.00%\n",
      "Sample: (\"@vogueglamGIRL Ah I know ! She is simply the best in The Sept Issue . My boyfriend 's aunt worked for Anna Wintor in NY .\", {'entities': [(102, 113, 'person')]})\n",
      "Training data size:  1470\n",
      "Starting model training\n",
      " 1 : {'ner': 196.6903260532981}\n",
      " 2 : {'ner': 177.45360075618638}\n",
      " 3 : {'ner': 166.5726435012601}\n",
      " 4 : {'ner': 156.40226888803224}\n",
      " 5 : {'ner': 151.4995999280466}\n",
      " 6 : {'ner': 142.71827948166344}\n",
      " 7 : {'ner': 119.7657555633445}\n",
      " 8 : {'ner': 116.02245190735623}\n",
      " 9 : {'ner': 103.71207135739431}\n",
      " 10 : {'ner': 114.14564675221692}\n",
      " 11 : {'ner': 76.04694154304939}\n",
      " 12 : {'ner': 73.80047218839076}\n",
      " 13 : {'ner': 95.17098001964972}\n",
      " 14 : {'ner': 78.15678980062536}\n",
      " 15 : {'ner': 75.36823760944635}\n",
      "Iteration 12 completed: Model saved as 'active_learning_ner_model_iteration_11'。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('active_learning_ner_model_iteration_11', True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_subsequent_iteration(\n",
    "    iteration=12,\n",
    "    model_name=\"active_learning_ner_model_iteration_10\",\n",
    "    export_file=\"manual_samples_12.json\",\n",
    "    annotated_file=\"manual_annotated_05_11.json\",\n",
    "    n_confidence=90,\n",
    "    n_random=10,\n",
    "    save_model_name=\"active_learning_ner_model_iteration_11\",\n",
    "    epochs=15,\n",
    "    dropout=0.2,\n",
    "    low_freq_categories =  [\"company\", \"facility\", \"geo-loc\", \"movie\", \"musicartist\",\n",
    "    \"person\", \"product\", \"sportsteam\", \"tvshow\", \"other\"]\n",
    ")\n",
    "###11-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b91cea96-8489-41c4-87af-dda20f69cde4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 13：Active Learning Iteration 13 - Sampling, Training\n",
      "Loaded optimizer state from 'active_learning_ner_model_iteration_11_optimizer.pkl'.\n",
      "\n",
      "Top 5 sentences with the lowest confidence：\n",
      "Sentence: to all my girls in london or travelling up tomorrow i love and miss you wish i could be there ! NEXT YEAR I WILL !, Confidence Score: 0.1\n",
      "Sentence: RT @IlliniCampusRec : Orange &amp; Blue Skate tonight , 7:30 - 9:30 pm ! Free for UI faculty/staff &amp; their immediate families . http://bit.ly/c1M8dr, Confidence Score: 0.1\n",
      "Sentence: I heart Park(ing ) Day ! Photo of meter shark : http://bit.ly/aQun1b, Confidence Score: 0.1\n",
      "Sentence: RT @Sexstrology : Pisces tend to escape into fantasy and day dreams . There they are free . &lt; yesss #wavesindexfingerintheair #AGREED ! ^10thpower, Confidence Score: 0.1\n",
      "Sentence: Gotta call mom 2 let her know it 's almost 3 . Yli gets out in 15 . Mom took the antenna home today &lt; connected it at home , finally ., Confidence Score: 0.1\n",
      "\n",
      "Low-Frequency Category Sampling Statistics:\n",
      "company: 2 samples\n",
      "Warning: The number of samples for category company (2) did not reach the target (30)，. Resampling is recommended.\n",
      "facility: 0 samples\n",
      "Warning: The number of samples for category facility (0) did not reach the target (30)，. Resampling is recommended.\n",
      "geo-loc: 3 samples\n",
      "Warning: The number of samples for category geo-loc (3) did not reach the target (30)，. Resampling is recommended.\n",
      "movie: 0 samples\n",
      "Warning: The number of samples for category movie (0) did not reach the target (30)，. Resampling is recommended.\n",
      "musicartist: 0 samples\n",
      "Warning: The number of samples for category musicartist (0) did not reach the target (30)，. Resampling is recommended.\n",
      "person: 2 samples\n",
      "Warning: The number of samples for category person (2) did not reach the target (30)，. Resampling is recommended.\n",
      "product: 0 samples\n",
      "Warning: The number of samples for category product (0) did not reach the target (30)，. Resampling is recommended.\n",
      "sportsteam: 0 samples\n",
      "Warning: The number of samples for category sportsteam (0) did not reach the target (30)，. Resampling is recommended.\n",
      "tvshow: 0 samples\n",
      "Warning: The number of samples for category tvshow (0) did not reach the target (30)，. Resampling is recommended.\n",
      "other: 2 samples\n",
      "Warning: The number of samples for category other (2) did not reach the target (30)，. Resampling is recommended.\n",
      "Exported 120 samples to 'manual_samples_13.json'. Please annotate 'manual_annotated_05_12.json'，然后继续下一次迭代。\n",
      "Loaded 100 new labeled samples.\n",
      "Total entities: 23, Ignored entities: 0, Ignored ratio: 0.00%\n",
      "Sample: (\"RT @dropolo Headed to da gump today alabama here I come &lt; &lt; come to shut it down broski .. fuck wit me..Parlae 's in house producer\", {'entities': [(36, 43, 'geo-loc')]})\n",
      "Training data size:  1570\n",
      "Starting model training\n",
      " 1 : {'ner': 110.52819195531033}\n",
      " 2 : {'ner': 118.04424112492819}\n",
      " 3 : {'ner': 115.77836974681334}\n",
      " 4 : {'ner': 104.53949216179268}\n",
      " 5 : {'ner': 103.66371898606982}\n",
      " 6 : {'ner': 111.57703475227966}\n",
      " 7 : {'ner': 97.77400163396224}\n",
      " 8 : {'ner': 64.18782076744424}\n",
      " 9 : {'ner': 85.72793348384108}\n",
      " 10 : {'ner': 78.92022309708322}\n",
      "Iteration 13 completed: Model saved as 'active_learning_ner_model_iteration_12'。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('active_learning_ner_model_iteration_12', True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_subsequent_iteration(\n",
    "    iteration=13,\n",
    "    model_name=\"active_learning_ner_model_iteration_11\",\n",
    "    export_file=\"manual_samples_13.json\",\n",
    "    annotated_file=\"manual_annotated_05_12.json\",\n",
    "    n_confidence=100,\n",
    "    n_random=20,\n",
    "    save_model_name=\"active_learning_ner_model_iteration_12\",\n",
    "    epochs=10,\n",
    "    dropout=0.2,\n",
    "    low_freq_categories =  [\"company\", \"facility\", \"geo-loc\", \"movie\", \"musicartist\",\n",
    "    \"person\", \"product\", \"sportsteam\", \"tvshow\", \"other\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "935931e6-50fb-40fb-b1f9-a9770cecb895",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 14：Active Learning Iteration 14 - Sampling, Training\n",
      "Loaded optimizer state from 'active_learning_ner_model_iteration_12_optimizer.pkl'.\n",
      "\n",
      "Top 5 sentences with the lowest confidence：\n",
      "Sentence: dear youtube why does it take so long to upload a video gggrrrrr, Confidence Score: 0.1\n",
      "Sentence: It 's the fescue turf grass aeration &amp; overseeding season for Atlanta &amp; N . Georgia . Do some evaluation &amp; planning beforehand , there 's time, Confidence Score: 0.1\n",
      "Sentence: Any 1 know when season 3 True Blood is back on the telly ?, Confidence Score: 0.1\n",
      "Sentence: last day of sorting pope visit to birmingham stuff out ..... hope it goes ok on sunday !!, Confidence Score: 0.1\n",
      "Sentence: @joejonas @nickjonas @kevinjonas @papajonas @greggarbo @johnlloydtaylor Rock to SECTION 204 tonight !!!!, Confidence Score: 0.1\n",
      "Exported 150 samples to 'manual_samples_14.json'. Please annotate 'manual_annotated_05_13.json'，然后继续下一次迭代。\n",
      "Loaded 120 new labeled samples.\n",
      "Total entities: 34, Ignored entities: 0, Ignored ratio: 0.00%\n",
      "Sample: ('to all my girls in london or travelling up tomorrow i love and miss you wish i could be there ! NEXT YEAR I WILL !', {'entities': [(19, 25, 'geo-loc')]})\n",
      "Training data size:  1690\n",
      "Starting model training\n",
      " 1 : {'ner': 150.2040259975555}\n",
      " 2 : {'ner': 135.29396747434248}\n",
      " 3 : {'ner': 130.9232237469795}\n",
      " 4 : {'ner': 109.58425855580964}\n",
      " 5 : {'ner': 116.06679000851706}\n",
      " 6 : {'ner': 94.89441566077008}\n",
      " 7 : {'ner': 84.32878798420344}\n",
      " 8 : {'ner': 77.37512292715715}\n",
      " 9 : {'ner': 86.9169003281966}\n",
      " 10 : {'ner': 102.61561693448016}\n",
      "Iteration 14 completed: Model saved as 'active_learning_ner_model_iteration_13'。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('active_learning_ner_model_iteration_13', True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_subsequent_iteration(\n",
    "    iteration=14,\n",
    "    model_name=\"active_learning_ner_model_iteration_12\",\n",
    "    export_file=\"manual_samples_14.json\",\n",
    "    annotated_file=\"manual_annotated_05_13.json\",\n",
    "    n_confidence=120,\n",
    "    n_random=30,\n",
    "    save_model_name=\"active_learning_ner_model_iteration_13\",\n",
    "    epochs=10,\n",
    "    dropout=0.2,\n",
    ")\n",
    "###13-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5be7ef6-56e3-4df1-9b13-f2f9cc7597f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_datasets(model, datasets, dataset_names):\n",
    "    results = {}\n",
    "    for dataset, name in zip(datasets, dataset_names):\n",
    "        scorer = spacy.scorer.Scorer()\n",
    "        examples = []\n",
    "        for text, annotations in dataset:\n",
    "            doc = model(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            examples.append(example)\n",
    "        scores = scorer.score(examples)\n",
    "        \n",
    "        # Print performance metrics\n",
    "        print(f\"\\n{name} Dataset Performance Metrics:\")\n",
    "        print(f\"Overall Metrics: Precision={scores['ents_p']:.2f}, Recall={scores['ents_r']:.2f},F1={scores['ents_f']:.2f}\")\n",
    "        print(\"\\nPer Entity Type:\")\n",
    "        for entity_type, metrics in scores[\"ents_per_type\"].items():\n",
    "            print(f\"{entity_type}: Precision={metrics['p']:.2f}, Recall={metrics['r']:.2f}, F1={metrics['f']:.2f}\")\n",
    "        \n",
    "        # Save results\n",
    "        results[name] = scores\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26221dd1-acc2-4898-a26e-4efcb17153a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Dataset Performance Metrics:\n",
      "Overall Metrics: Precision=0.96, Recall=0.76,F1=0.85\n",
      "\n",
      "Per Entity Type:\n",
      "geo-loc: Precision=0.97, Recall=0.76, F1=0.85\n",
      "facility: Precision=0.97, Recall=0.71, F1=0.82\n",
      "movie: Precision=0.96, Recall=0.76, F1=0.85\n",
      "company: Precision=0.97, Recall=0.81, F1=0.88\n",
      "product: Precision=0.96, Recall=0.84, F1=0.90\n",
      "person: Precision=0.98, Recall=0.78, F1=0.87\n",
      "other: Precision=0.92, Recall=0.76, F1=0.83\n",
      "sportsteam: Precision=0.95, Recall=0.75, F1=0.84\n",
      "tvshow: Precision=0.91, Recall=0.62, F1=0.74\n",
      "musicartist: Precision=0.93, Recall=0.69, F1=0.79\n",
      "\n",
      "Dev Dataset Performance Metrics:\n",
      "Overall Metrics: Precision=0.30, Recall=0.12,F1=0.18\n",
      "\n",
      "Per Entity Type:\n",
      "other: Precision=0.18, Recall=0.10, F1=0.13\n",
      "company: Precision=0.27, Recall=0.21, F1=0.23\n",
      "geo-loc: Precision=0.44, Recall=0.24, F1=0.31\n",
      "product: Precision=0.12, Recall=0.05, F1=0.07\n",
      "facility: Precision=0.14, Recall=0.05, F1=0.08\n",
      "person: Precision=0.37, Recall=0.15, F1=0.21\n",
      "sportsteam: Precision=0.60, Recall=0.04, F1=0.08\n",
      "musicartist: Precision=0.00, Recall=0.00, F1=0.00\n",
      "movie: Precision=0.50, Recall=0.07, F1=0.12\n",
      "tvshow: Precision=0.00, Recall=0.00, F1=0.00\n",
      "\n",
      "Test Dataset Performance Metrics:\n",
      "Overall Metrics: Precision=0.35, Recall=0.15,F1=0.21\n",
      "\n",
      "Per Entity Type:\n",
      "other: Precision=0.17, Recall=0.08, F1=0.11\n",
      "movie: Precision=0.00, Recall=0.00, F1=0.00\n",
      "person: Precision=0.25, Recall=0.18, F1=0.21\n",
      "geo-loc: Precision=0.55, Recall=0.30, F1=0.39\n",
      "company: Precision=0.44, Recall=0.12, F1=0.19\n",
      "product: Precision=0.08, Recall=0.03, F1=0.04\n",
      "musicartist: Precision=0.06, Recall=0.01, F1=0.01\n",
      "sportsteam: Precision=0.08, Recall=0.01, F1=0.01\n",
      "facility: Precision=0.42, Recall=0.15, F1=0.22\n",
      "tvshow: Precision=0.00, Recall=0.00, F1=0.00\n",
      "Test results have been saved to 'evaluation_results.json'。\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://raw.githubusercontent.com/aritter/twitter_nlp/master/data/annotated/wnut16/data/\"\n",
    "train_url = f\"{base_url}train\"\n",
    "dev_url = f\"{base_url}dev\"\n",
    "test_url = f\"{base_url}test\"\n",
    "\n",
    "_, train_data = load_data(train_url)\n",
    "_, dev_data = load_data(dev_url)\n",
    "_, test_data = load_data(test_url)\n",
    "\n",
    "model = spacy.load(\"active_learning_ner_model_iteration_10\")\n",
    "\n",
    "datasets = [train_data, dev_data, test_data]\n",
    "dataset_names = [\"Train\", \"Dev\", \"Test\"]\n",
    "results = evaluate_model_on_datasets(model, datasets, dataset_names)\n",
    "\n",
    "with open(\"evaluation_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False)\n",
    "print(\"Test results have been saved to 'evaluation_results.json'。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9411a93-79a6-4d4a-9ad9-2c8b9f2b7bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
